# Configuration de l'Assistant IA
# Tu peux modifier ce fichier sans toucher au code !

# === LLM (Cerveau) ===
llm:
  # Quel type de LLM utiliser : "ollama" ou "llamacpp"
  # - "ollama"   : Simple, modÃ¨les prÃ©-packagÃ©s (llama3.2, mistral, etc.)
  # - "llamacpp" : Flexible, supporte les modÃ¨les vision (Jan-v2-VL, LLaVA, etc.)
  provider: "llamacpp"
  
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ¦™ LLAMA.CPP SERVER (pour Jan-v2-VL et modÃ¨les vision)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Jan-v2-VL-high est un modÃ¨le "thinking" (8B params) qui rÃ©flÃ©chit avant
  # de rÃ©pondre. Il utilise Vulkan pour le GPU (pas besoin de CUDA Toolkit).
  # 
  # Lancer le serveur avant de dÃ©marrer l'assistant:
  #   ./scripts/start_llm_server.sh
  # ou manuellement:
  #   cd ~/tools/llama-cpp && LD_LIBRARY_PATH="$PWD" ./llama-server \
  #     --model ~/models/jan-v2-vl-high/Jan-v2-VL-high-Q4_K_M.gguf \
  #     --mmproj ~/models/jan-v2-vl-high/mmproj-Jan-v2-VL-high.gguf \
  #     --host 0.0.0.0 --port 8080 --ctx-size 8192 --n-gpu-layers 99 \
  #     --jinja --no-context-shift
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  llamacpp:
    base_url: "http://localhost:8080"
    model_name: "jan-v2-vl-high"
    # ParamÃ¨tres recommandÃ©s pour Jan-v2-VL (modÃ¨le "thinking")
    max_tokens: 2048        # Les modÃ¨les thinking ont besoin de plus de tokens
    temperature: 1.0        # RecommandÃ© par Jan pour ce modÃ¨le
    top_p: 0.95             # Top-p sampling
    top_k: 20               # Top-k sampling  
    presence_penalty: 1.5   # RÃ©duit les rÃ©pÃ©titions
  
  # Configuration Ollama (fallback)
  ollama:
    model: "llama3.2:3b"      # ModÃ¨le Ã  utiliser
    base_url: "http://localhost:11434"

# === PersonnalitÃ© de l'assistant ===
character:
  name: "Aria"
  # Le "system prompt" dÃ©finit la personnalitÃ© de l'IA
  system_prompt: |
    Tu es Aria, une assistante IA amicale et serviable.
    Tu rÃ©ponds de maniÃ¨re concise et claire en franÃ§ais.
    Tu es enthousiaste et positive.
    Tu aides l'utilisateur dans ses tÃ¢ches quotidiennes.

# === TTS (Voix) ===
tts:
  # Provider TTS par dÃ©faut : "kokoro" (local, naturel) ou "edge" (cloud, gratuit)
  provider: "kokoro"
  
  # --- Kokoro TTS (100% local, haute qualitÃ©) ---
  # Voix disponibles: ff_siwis (FR), af_heart (US), bf_emma (UK), jf_alpha (JP)
  kokoro_voice: "ff_siwis"
  
  # --- Edge TTS (cloud Microsoft, fallback) ---
  voice: "fr-FR-DeniseNeural"
  # Autres voix Edge:
  # - fr-FR-HenriNeural (homme franÃ§ais)
  # - en-US-JennyNeural (femme amÃ©ricaine)  
  # - ja-JP-NanamiNeural (femme japonaise)
  
  rate: "+20%"   # Vitesse Edge TTS (-50% Ã  +100%)
  pitch: "+0Hz"  # Hauteur Edge TTS (-50Hz Ã  +50Hz)

# === ASR (Reconnaissance vocale) ===
asr:
  # Provider ASR disponibles :
  # - "whisper"  : Local, compatible CPU/GPU, bon pour le franÃ§ais avec modÃ¨les distillÃ©s
  # - "canary"   : NVIDIA Canary 1B v2, meilleur pour longs audios (6GB+ VRAM requis)
  # - "parakeet" : NVIDIA Parakeet TDT 0.6B v3, rapide et prÃ©cis (peut tourner sur CPU)
  # âš ï¸ Avec Jan-v2-VL (~7GB VRAM), utiliser ASR sur CPU pour Ã©viter OOM
  provider: "parakeet"
  
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ¦œ PARAKEET TDT 0.6B V3 (NVIDIA) - Rapide et prÃ©cis â­ RECOMMANDÃ‰
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # - 600M params, ~2GB VRAM (lÃ©ger!)
  # - Token-and-Duration Transducer (TDT) architecture
  # - Excellent sur audios courts ET longs
  # - 25 langues supportÃ©es (dont franÃ§ais, anglais, espagnol...)
  # - DÃ©tection automatique de la langue
  # - Plus rapide que Canary et Whisper large
  # - REQUIERT un GPU NVIDIA
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ¤ CANARY 1B V2 (NVIDIA) - State-of-the-art ASR
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # - Meilleure prÃ©cision que Whisper (WER ~5% pour le franÃ§ais)
  # - 25 langues europÃ©ennes supportÃ©es
  # - Ponctuation et majuscules automatiques
  # - REQUIERT un GPU NVIDIA avec 6GB+ VRAM
  # - Plus rapide que Whisper sur GPU
  # âš ï¸ Moins bon sur les phrases courtes conversationnelles
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  # ModÃ¨les Whisper disponibles (si provider: "whisper"):
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # MODÃˆLES STANDARD (tÃ©lÃ©chargement auto depuis OpenAI):
  # - tiny   : 39M params, trÃ¨s rapide, faible qualitÃ©
  # - base   : 74M params, rapide, qualitÃ© moyenne
  # - small  : 244M params, BON COMPROMIS! LÃ©ger + bonne qualitÃ©
  # - medium : 769M params, lent, trÃ¨s bonne qualitÃ©
  # - large-v3: 1.5B params, trÃ¨s lent, meilleure qualitÃ©
  # - turbo  : 809M params, 6x plus rapide que large-v3 (mais hallucine parfois)
  # - distil-large-v3: 756M params, rapide mais ANGLAIS SEULEMENT
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ‡«ğŸ‡· MODÃˆLES FRANÃ‡AIS OPTIMISÃ‰S (tÃ©lÃ©chargÃ©s dans models/, gitignored):
  # - french-distil-dec4 : 0.8B params, Excellente qualitÃ© FR, ~500MB download
  # - french-distil-dec2 : 0.8B params, Plus rapide, trÃ¨s bonne qualitÃ© FR
  # Ces modÃ¨les sont fine-tunÃ©s sur 2500+ heures de franÃ§ais par bofenghuang.
  # WER: 8.37% (bien meilleur que Whisper standard pour le franÃ§ais)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  model_size: "french-distil-dec2"  # ModÃ¨le franÃ§ais optimisÃ© - rapide et prÃ©cis
  
  # Langue: "fr", "en", "auto" ou vide "" pour auto-dÃ©tection
  # Pour les modÃ¨les franÃ§ais, laisser "fr" pour de meilleurs rÃ©sultats.
  language: "fr"
  
  # Prompt pour guider la transcription (aide mÃªme avec langue forcÃ©e)
  # Donne du contexte sur le style de conversation attendu.
  # Note: UtilisÃ© uniquement par Whisper, Canary n'utilise pas de prompt.
  prompt: "Transcription d'une conversation en franÃ§ais avec une assistante IA."
  
  # Device: "cpu" ou "cuda" (GPU)
  # âš ï¸ Avec Jan-v2-VL sur GPU, utiliser "cpu" pour Whisper afin d'Ã©viter OOM
  # Whisper french-distil-dec2 sur CPU est assez rapide (~1-2s par phrase)
  device: "cpu"
