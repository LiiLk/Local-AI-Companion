"""
Point d'entr√©e principal de l'assistant IA avec voix.

L'IA r√©pond en texte ET en voix simultan√©ment !
Le TTS se d√©clenche phrase par phrase pour une latence minimale.

Usage:
    python main.py                    # Mode texte uniquement
    python main.py --voice            # Mode texte + voix (Kokoro par d√©faut)
    python main.py --voice --tts edge # Mode texte + voix Edge TTS
    python main.py --voice --listen   # Mode conversation vocale compl√®te
"""

import asyncio
import argparse
import subprocess
import tempfile
import re
import yaml
from pathlib import Path

from src.llm import OllamaLLM
from src.llm.base import Message
from src.tts import EdgeTTSProvider, KokoroProvider, XTTSProvider
from src.tts.base import BaseTTS
from src.asr import RealtimeWhisperProvider
from src.asr.base import BaseASR


def load_config() -> dict:
    """Charge la configuration depuis config.yaml"""
    config_path = Path(__file__).parent / "config" / "config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def play_audio(audio_path: Path) -> subprocess.Popen:
    """
    Joue un fichier audio en arri√®re-plan.
    
    Supporte WAV (Kokoro) et MP3 (Edge TTS).
    Utilise mpv, ffplay ou aplay selon ce qui est disponible.
    Retourne le processus pour pouvoir l'arr√™ter si besoin.
    """
    players = [
        ["ffplay", "-nodisp", "-autoexit", "-loglevel", "quiet", str(audio_path)],
        ["mpv", "--no-terminal", "--no-video", str(audio_path)],
        ["aplay", str(audio_path)],  # WAV uniquement
    ]
    
    for player_cmd in players:
        try:
            # Lancer en arri√®re-plan, sans output
            process = subprocess.Popen(
                player_cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            return process
        except FileNotFoundError:
            continue
    
    print("\n‚ö†Ô∏è  Aucun lecteur audio trouv√© (ffplay, mpv, aplay)")
    return None


def create_tts(provider: str, tts_config: dict) -> BaseTTS:
    """
    Cr√©e le provider TTS appropri√©.
    
    Args:
        provider: "f5tts", "kokoro" ou "edge"
        tts_config: Configuration TTS depuis config.yaml
        
    Returns:
        Instance du provider TTS
    """
    if provider == "xtts":
        # XTTS v2 - Voice cloning multilingue de qualit√©
        from pathlib import Path
        xtts_config = tts_config.get("xtts", {})
        speaker_wav = xtts_config.get("speaker_wav")
        
        # Expand speaker_wav path if provided
        if speaker_wav:
            speaker_wav = str(Path(speaker_wav).expanduser())
        
        return XTTSProvider(
            language=xtts_config.get("language", "fr"),
            speaker=xtts_config.get("speaker", "Claribel Dervla"),
            speaker_wav=speaker_wav,
            device=xtts_config.get("device"),  # None = auto-detect
        )
    elif provider == "kokoro":
        # Kokoro - TTS local haute qualit√©
        voice = tts_config.get("kokoro_voice", "ff_siwis")
        return KokoroProvider(voice=voice)
    else:
        # Edge TTS - Cloud Microsoft (fallback)
        voice = tts_config.get("voice", "fr-FR-DeniseNeural")
        rate = tts_config.get("rate", "+20%")
        pitch = tts_config.get("pitch", "+0Hz")
        return EdgeTTSProvider(voice=voice, rate=rate, pitch=pitch)


def create_asr(asr_config: dict) -> RealtimeWhisperProvider:
    """
    Cr√©e le provider ASR (Speech-to-Text).
    
    Args:
        asr_config: Configuration ASR depuis config.yaml
        
    Returns:
        Instance du provider ASR
    """
    model_size = asr_config.get("model_size", "base")
    device = asr_config.get("device", "cpu")  # CPU par d√©faut (cuDNN issues)
    
    return RealtimeWhisperProvider(
        model_size=model_size,
        device=device
    )


def split_into_sentences(text: str) -> list[str]:
    """
    D√©coupe le texte en phrases pour le TTS.
    
    On veut des phrases compl√®tes pour un TTS naturel.
    """
    # Pattern pour d√©tecter les fins de phrases
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]


async def speak_text(tts: BaseTTS, text: str, temp_dir: Path) -> subprocess.Popen | None:
    """
    Synth√©tise et joue le texte.
    
    Returns:
        Le processus audio pour pouvoir attendre qu'il finisse
    """
    if not text.strip():
        return None
    
    # Extension selon le type de TTS
    # XTTS et Kokoro g√©n√®rent du WAV, Edge TTS g√©n√®re du MP3
    ext = ".wav" if isinstance(tts, (KokoroProvider, XTTSProvider)) else ".mp3"
    
    # G√©n√©rer un fichier temporaire unique
    audio_file = temp_dir / f"speech_{hash(text) % 10000}{ext}"
    
    # Synth√©tiser
    await tts.synthesize(text, audio_file)
    
    # Jouer
    return play_audio(audio_file)


async def main():
    """
    Boucle principale du chatbot avec support vocal.
    """
    # Parser les arguments
    parser = argparse.ArgumentParser(description="Local AI Companion")
    parser.add_argument("--voice", "-v", action="store_true", 
                       help="Activer la synth√®se vocale")
    parser.add_argument("--tts", type=str, default="kokoro",
                       choices=["xtts", "kokoro", "edge"],
                       help="Provider TTS: xtts (voice cloning), kokoro (local) ou edge (cloud)")
    parser.add_argument("--listen", "-l", action="store_true",
                       help="Activer l'√©coute vocale (microphone)")
    parser.add_argument("--asr-model", type=str, default="base",
                       choices=["tiny", "base", "small", "medium", "large-v3"],
                       help="Taille du mod√®le Whisper pour l'ASR")
    args = parser.parse_args()
    
    # Si --listen est activ√©, activer aussi --voice automatiquement
    if args.listen:
        args.voice = True
    
    # Charger la configuration
    config = load_config()
    llm_config = config["llm"]["ollama"]
    character = config["character"]
    tts_config = config.get("tts", {})
    asr_config = config.get("asr", {})
    
    # Surcharger avec les arguments CLI
    asr_config["model_size"] = args.asr_model
    
    print("=" * 50)
    print(f"ü§ñ {character['name']} - Local AI Companion")
    print("=" * 50)
    
    if args.listen:
        print("üé§ Mode CONVERSATION VOCALE activ√©")
        print("   Parlez dans votre micro, l'IA vous r√©pondra √† voix haute !")
    elif args.voice:
        tts_names = {
            "openaudio": "OpenAudio S1-mini (#1 quality)",
            "kokoro": "Kokoro (local)",
            "edge": "Edge TTS (cloud)"
        }
        tts_name = tts_names.get(args.tts, args.tts)
        print(f"üîä Mode vocal ACTIV√â - {tts_name}")
    else:
        print("üîá Mode texte (utilise --voice ou --listen)")
    
    print("\nCommandes: 'quit', 'clear', 'voice on', 'voice off', 'listen on', 'listen off'")
    print()
    
    # Cr√©er le client LLM
    llm = OllamaLLM(
        model=llm_config["model"],
        base_url=llm_config["base_url"]
    )
    
    # Cr√©er le TTS si mode vocal
    tts = None
    tts_provider = args.tts
    temp_dir = None
    
    if args.voice:
        tts = create_tts(tts_provider, tts_config)
        temp_dir = Path(tempfile.mkdtemp(prefix="ai_companion_"))
        
        voice_info = tts.voice if hasattr(tts, 'voice') else "default"
        print(f"üîä Voix TTS: {voice_info}")
    
    # Cr√©er l'ASR si mode √©coute
    asr = None
    listen_mode = args.listen
    
    if listen_mode:
        asr = create_asr(asr_config)
        print(f"üé§ ASR: Whisper {args.asr_model}")
    
    print()
    
    # Historique de la conversation
    messages: list[Message] = [
        Message(role="system", content=character["system_prompt"])
    ]
    
    audio_processes: list[subprocess.Popen] = []
    
    try:
        while True:
            # Attendre que les audios pr√©c√©dents finissent
            for proc in audio_processes:
                if proc:
                    proc.wait()
            audio_processes.clear()
            
            # 1. Obtenir l'entr√©e utilisateur (texte ou voix)
            user_input = None
            
            if listen_mode and asr:
                # Mode √©coute vocale
                print("\nüé§ [Parlez maintenant... ou tapez du texte]")
                
                # On utilise un syst√®me hybride: 
                # - Soit l'utilisateur parle (ASR)
                # - Soit il tape du texte (fallback)
                try:
                    # Essayer d'√©couter pendant 10 secondes max
                    result = await asr.listen_once(timeout=10.0)
                    user_input = result.text.strip()
                    
                    if user_input:
                        print(f"üë§ Toi (voix): {user_input}")
                    else:
                        print("   (Pas de parole d√©tect√©e, tapez votre message)")
                        user_input = input("üë§ Toi: ").strip()
                        
                except KeyboardInterrupt:
                    # L'utilisateur a appuy√© sur Ctrl+C pendant l'√©coute
                    print("\n   (√âcoute annul√©e)")
                    user_input = input("üë§ Toi: ").strip()
                except Exception as e:
                    print(f"\n   ‚ö†Ô∏è Erreur ASR: {e}")
                    user_input = input("üë§ Toi: ").strip()
            else:
                # Mode texte classique
                try:
                    user_input = input("\nüë§ Toi: ").strip()
                except EOFError:
                    break
            
            if not user_input:
                continue
            
            # Commandes sp√©ciales
            if user_input.lower() == "quit":
                print("\nüëã √Ä bient√¥t !")
                break
            if user_input.lower() == "clear":
                messages = [Message(role="system", content=character["system_prompt"])]
                print("üóëÔ∏è  Historique effac√© !")
                continue
            if user_input.lower() == "voice on":
                if not tts:
                    tts = create_tts(tts_provider, tts_config)
                    temp_dir = Path(tempfile.mkdtemp(prefix="ai_companion_"))
                print("üîä Mode vocal activ√© !")
                continue
            if user_input.lower() == "voice off":
                tts = None
                print("üîá Mode vocal d√©sactiv√© !")
                continue
            if user_input.lower() == "listen on":
                if not asr:
                    asr = create_asr(asr_config)
                listen_mode = True
                print("üé§ Mode √©coute activ√© !")
                continue
            if user_input.lower() == "listen off":
                listen_mode = False
                print("‚å®Ô∏è  Mode √©coute d√©sactiv√© (texte uniquement)")
                continue
            
            # 2. Ajouter le message utilisateur √† l'historique
            messages.append(Message(role="user", content=user_input))
            
            # 3. Obtenir la r√©ponse du LLM (avec streaming)
            print(f"\nü§ñ {character['name']}: ", end="", flush=True)
            
            full_response = ""
            
            async for chunk in llm.chat_stream(messages):
                print(chunk, end="", flush=True)
                full_response += chunk
            
            print()  # Nouvelle ligne
            
            # 4. TTS sur la r√©ponse compl√®te
            if tts and full_response.strip():
                proc = await speak_text(tts, full_response, temp_dir)
                if proc:
                    audio_processes.append(proc)
            
            # 5. Ajouter la r√©ponse √† l'historique
            messages.append(Message(role="assistant", content=full_response))
            
    finally:
        for proc in audio_processes:
            if proc:
                proc.wait()
        
        if temp_dir and temp_dir.exists():
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        await llm.close()


if __name__ == "__main__":
    asyncio.run(main())
